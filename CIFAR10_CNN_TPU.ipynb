{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10-CNN_TPU",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paul028/myJupyterNotebook/blob/master/CIFAR10_CNN_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ue90VvMQV_UO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Image CLassifier trained using CIFAR10 dataset using 3-layer Convolutional Neural Network**\n",
        "\n",
        "\n",
        "1. This model was trained Using TPU from google Colab\n",
        "2. Implements Data Augmentation\n",
        "3. Implements Regularization Technique \n",
        "  *  Dropout\n",
        "  *  Kernel Regularizer\n",
        "  * Batch Normalization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Fvb-PvEpXQub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Import all the Necessary Libraries"
      ]
    },
    {
      "metadata": {
        "id": "b3PGh4qDX0Ay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Input\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import toimage\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LCh7QEQQX3Yg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Call the Address of the TPU"
      ]
    },
    {
      "metadata": {
        "id": "8hfEfr7EX5Yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A7lrsjApYCVX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Prepare the Dataset"
      ]
    },
    {
      "metadata": {
        "id": "KqQTN52wYEAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes =  len(np.unique(y_train))\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zJuUBNvNYUqO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Dataset Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "bwMt2nwLYYW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=128):\n",
        "    # Show 9 images\n",
        "    for i in range(0, 9):\n",
        "        plt.subplot(330 + 1 + i)\n",
        "        plt.imshow(toimage(X_batch[i].reshape(32, 32, 3)))\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tnJb3y7lYktT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. Initialize the Parameters"
      ]
    },
    {
      "metadata": {
        "id": "mo9H4nciYkHD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "weight_decay = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ofxA3bMYtjK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "6. Prepare the Model"
      ]
    },
    {
      "metadata": {
        "id": "EtWgVTMXYttK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "opt_rms = tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWAaCKaGY9Ee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "7. Define a Function for Changing Learning Rate"
      ]
    },
    {
      "metadata": {
        "id": "rQP_CR54Y-I_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbezGUlBZB-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "8. Convert the Model to TPU"
      ]
    },
    {
      "metadata": {
        "id": "5aLKAAexZFZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "model,\n",
        "strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEvt7ShjZM9i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "9. Train the Network"
      ]
    },
    {
      "metadata": {
        "id": "KGDOm6buep_Z",
        "colab_type": "code",
        "outputId": "5d91107f-2508-4f8c-c0c0-dee22cf3e271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8418
        }
      },
      "cell_type": "code",
      "source": [
        "tpu_model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_12 (B (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_13 (B (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_14 (B (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_15 (B (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_16 (B (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_17 (B (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.34.91.218:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4301194919384501650)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9049157572559682020)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7003042714619960513)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5395252218734142924)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 11005432836701920464)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10114078223263709694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12853116236621313143)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3781250911297701929)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16508279647462104902)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8895999580285025702)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 3671194023851832492)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning RMSprop {'lr': 0.0010000000474974513, 'rho': 0.8999999761581421, 'decay': 9.999999974752427e-07, 'epsilon': 1e-07}\n",
            "INFO:tensorflow:Cloning RMSprop {'lr': 0.0010000000474974513, 'rho': 0.8999999761581421, 'decay': 9.999999974752427e-07, 'epsilon': 1e-07}\n",
            "Epoch 1/125\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id_40'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning RMSprop {'lr': 0.0010000000474974513, 'rho': 0.8999999761581421, 'decay': 9.999999974752427e-07, 'epsilon': 1e-07}\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.RMSprop object at 0x7f1c4fce52b0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 10.73765516281128 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU rho: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU decay: 9.999999974752427e-07 {1e-06}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "180/782 [=====>........................] - ETA: 2:28 - loss: 2.7364 - acc: 0.2827INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id_40'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.RMSprop object at 0x7f1c4fce52b0> [<tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c1070b8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c107400>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c1077f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c11fe10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c066f60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c052630>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4c01aeb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bfbff60>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bf29cc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bf13c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bede470>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4be80f98>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4be47dd8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bdd8e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bd9ee80>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bd40da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bd08f28>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bc709e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bc3ac18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bc5dba8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bc24e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bb32908>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bafe208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bb1bd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4bae4b00>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f1c4ba51d68>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.650966882705688 secs\n",
            "781/782 [============================>.] - ETA: 0s - loss: 2.0480 - acc: 0.3920INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(8, 32, 32, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(8, 10), dtype=tf.float32, name='dense_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning RMSprop {'lr': 0.0010000000474974513, 'rho': 0.8999999761581421, 'decay': 9.999999974752427e-07, 'epsilon': 1e-07}\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.RMSprop object at 0x7f1c490d7cc0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 11.222867012023926 secs\n",
            " 9984/10000 [============================>.] - ETA: 0s - loss: 1.3399 - acc: 0.5705INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(2,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(2, 32, 32, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(2, 10), dtype=tf.float32, name='dense_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.RMSprop object at 0x7f1c490d7cc0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.279120206832886 secs\n",
            "10000/10000 [==============================] - 26s 3ms/sample - loss: 1.3400 - acc: 0.5703\n",
            "782/782 [==============================] - 115s 148ms/step - loss: 2.0476 - acc: 0.3920 - val_loss: 1.3402 - val_acc: 0.5703\n",
            "Epoch 2/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 1.0606 - acc: 0.6624\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 1.3365 - acc: 0.5618 - val_loss: 1.0602 - val_acc: 0.6624\n",
            "Epoch 3/125\n",
            "10000/10000 [==============================] - 2s 245us/sample - loss: 1.0632 - acc: 0.6680\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 1.1352 - acc: 0.6317 - val_loss: 1.0632 - val_acc: 0.6680\n",
            "Epoch 4/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.9018 - acc: 0.7301\n",
            "782/782 [==============================] - 30s 38ms/step - loss: 1.0252 - acc: 0.6724 - val_loss: 0.9037 - val_acc: 0.7301\n",
            "Epoch 5/125\n",
            "10000/10000 [==============================] - 2s 237us/sample - loss: 0.9003 - acc: 0.7348\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.9611 - acc: 0.6986 - val_loss: 0.9010 - val_acc: 0.7348\n",
            "Epoch 6/125\n",
            "10000/10000 [==============================] - 2s 238us/sample - loss: 0.9784 - acc: 0.7232\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.9207 - acc: 0.7139 - val_loss: 0.9788 - val_acc: 0.7232\n",
            "Epoch 7/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.7969 - acc: 0.7725\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.8844 - acc: 0.7292 - val_loss: 0.7978 - val_acc: 0.7725\n",
            "Epoch 8/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.7223 - acc: 0.7945\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.8567 - acc: 0.7424 - val_loss: 0.7222 - val_acc: 0.7945\n",
            "Epoch 9/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.8868 - acc: 0.7579\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.8371 - acc: 0.7502 - val_loss: 0.8886 - val_acc: 0.7579\n",
            "Epoch 10/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.7170 - acc: 0.7966\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.8231 - acc: 0.7558 - val_loss: 0.7167 - val_acc: 0.7966\n",
            "Epoch 11/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.7534 - acc: 0.7882\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.8047 - acc: 0.7643 - val_loss: 0.7537 - val_acc: 0.7882\n",
            "Epoch 12/125\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 0.8851 - acc: 0.7568\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.7900 - acc: 0.7713 - val_loss: 0.8857 - val_acc: 0.7568\n",
            "Epoch 13/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.6802 - acc: 0.8154\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7848 - acc: 0.7728 - val_loss: 0.6802 - val_acc: 0.8154\n",
            "Epoch 14/125\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 0.7707 - acc: 0.7894\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7733 - acc: 0.7780 - val_loss: 0.7704 - val_acc: 0.7894\n",
            "Epoch 15/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.7711 - acc: 0.7948\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.7691 - acc: 0.7806 - val_loss: 0.7704 - val_acc: 0.7948\n",
            "Epoch 16/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.7741 - acc: 0.7919\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7495 - acc: 0.7882 - val_loss: 0.7737 - val_acc: 0.7919\n",
            "Epoch 17/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.7145 - acc: 0.8108\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7533 - acc: 0.7874 - val_loss: 0.7144 - val_acc: 0.8108\n",
            "Epoch 18/125\n",
            "10000/10000 [==============================] - 2s 222us/sample - loss: 0.7323 - acc: 0.8042\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.7484 - acc: 0.7889 - val_loss: 0.7315 - val_acc: 0.8042\n",
            "Epoch 19/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.6764 - acc: 0.8249\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7398 - acc: 0.7940 - val_loss: 0.6757 - val_acc: 0.8249\n",
            "Epoch 20/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.7210 - acc: 0.8093\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7340 - acc: 0.7961 - val_loss: 0.7217 - val_acc: 0.8093\n",
            "Epoch 21/125\n",
            "10000/10000 [==============================] - 2s 223us/sample - loss: 0.6739 - acc: 0.8235\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7322 - acc: 0.7963 - val_loss: 0.6747 - val_acc: 0.8235\n",
            "Epoch 22/125\n",
            "10000/10000 [==============================] - 2s 227us/sample - loss: 0.6994 - acc: 0.8211\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7272 - acc: 0.7992 - val_loss: 0.6992 - val_acc: 0.8211\n",
            "Epoch 23/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6528 - acc: 0.8265\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7213 - acc: 0.8042 - val_loss: 0.6530 - val_acc: 0.8265\n",
            "Epoch 24/125\n",
            "10000/10000 [==============================] - 2s 240us/sample - loss: 0.6450 - acc: 0.8345\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7209 - acc: 0.8032 - val_loss: 0.6453 - val_acc: 0.8345\n",
            "Epoch 25/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.7402 - acc: 0.8074\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.7165 - acc: 0.8047 - val_loss: 0.7408 - val_acc: 0.8074\n",
            "Epoch 26/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.6734 - acc: 0.8309\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.7167 - acc: 0.8053 - val_loss: 0.6731 - val_acc: 0.8309\n",
            "Epoch 27/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.6782 - acc: 0.8287\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.7130 - acc: 0.8068 - val_loss: 0.6770 - val_acc: 0.8287\n",
            "Epoch 28/125\n",
            "10000/10000 [==============================] - 2s 227us/sample - loss: 0.6959 - acc: 0.8249\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.7062 - acc: 0.8105 - val_loss: 0.6960 - val_acc: 0.8249\n",
            "Epoch 29/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.6919 - acc: 0.8259\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.7054 - acc: 0.8122 - val_loss: 0.6915 - val_acc: 0.8259\n",
            "Epoch 30/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6231 - acc: 0.8443\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.7028 - acc: 0.8114 - val_loss: 0.6227 - val_acc: 0.8443\n",
            "Epoch 31/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.6527 - acc: 0.8360\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.7018 - acc: 0.8129 - val_loss: 0.6522 - val_acc: 0.8360\n",
            "Epoch 32/125\n",
            "10000/10000 [==============================] - 2s 220us/sample - loss: 0.6613 - acc: 0.8351\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6944 - acc: 0.8162 - val_loss: 0.6624 - val_acc: 0.8351\n",
            "Epoch 33/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.6256 - acc: 0.8439\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6935 - acc: 0.8147 - val_loss: 0.6248 - val_acc: 0.8439\n",
            "Epoch 34/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.6247 - acc: 0.8465\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6925 - acc: 0.8150 - val_loss: 0.6243 - val_acc: 0.8465\n",
            "Epoch 35/125\n",
            "10000/10000 [==============================] - 2s 217us/sample - loss: 0.6936 - acc: 0.8297\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6879 - acc: 0.8178 - val_loss: 0.6936 - val_acc: 0.8297\n",
            "Epoch 36/125\n",
            "10000/10000 [==============================] - 3s 254us/sample - loss: 0.6568 - acc: 0.8339\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6908 - acc: 0.8161 - val_loss: 0.6561 - val_acc: 0.8339\n",
            "Epoch 37/125\n",
            "10000/10000 [==============================] - 2s 225us/sample - loss: 0.7210 - acc: 0.8090\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6871 - acc: 0.8174 - val_loss: 0.7225 - val_acc: 0.8090\n",
            "Epoch 38/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.6316 - acc: 0.8437\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6888 - acc: 0.8172 - val_loss: 0.6318 - val_acc: 0.8437\n",
            "Epoch 39/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.6615 - acc: 0.8328\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.6822 - acc: 0.8208 - val_loss: 0.6621 - val_acc: 0.8328\n",
            "Epoch 40/125\n",
            "10000/10000 [==============================] - 2s 225us/sample - loss: 0.6462 - acc: 0.8385\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6805 - acc: 0.8211 - val_loss: 0.6465 - val_acc: 0.8385\n",
            "Epoch 41/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.6188 - acc: 0.8484\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6821 - acc: 0.8214 - val_loss: 0.6180 - val_acc: 0.8484\n",
            "Epoch 42/125\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.6233 - acc: 0.8514\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6794 - acc: 0.8206 - val_loss: 0.6239 - val_acc: 0.8514\n",
            "Epoch 43/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.7318 - acc: 0.8125\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6793 - acc: 0.8211 - val_loss: 0.7320 - val_acc: 0.8125\n",
            "Epoch 44/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.6577 - acc: 0.8376\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6742 - acc: 0.8214 - val_loss: 0.6576 - val_acc: 0.8376\n",
            "Epoch 45/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.5929 - acc: 0.8558\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6736 - acc: 0.8219 - val_loss: 0.5927 - val_acc: 0.8558\n",
            "Epoch 46/125\n",
            "10000/10000 [==============================] - 2s 222us/sample - loss: 0.6608 - acc: 0.8376\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6728 - acc: 0.8254 - val_loss: 0.6607 - val_acc: 0.8376\n",
            "Epoch 47/125\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 0.5878 - acc: 0.8544\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6730 - acc: 0.8268 - val_loss: 0.5873 - val_acc: 0.8544\n",
            "Epoch 48/125\n",
            "10000/10000 [==============================] - 2s 222us/sample - loss: 0.6583 - acc: 0.8350\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6752 - acc: 0.8249 - val_loss: 0.6585 - val_acc: 0.8350\n",
            "Epoch 49/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.6419 - acc: 0.8380\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6682 - acc: 0.8263 - val_loss: 0.6425 - val_acc: 0.8380\n",
            "Epoch 50/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.6671 - acc: 0.8344\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6705 - acc: 0.8254 - val_loss: 0.6680 - val_acc: 0.8344\n",
            "Epoch 51/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.6746 - acc: 0.8313\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6691 - acc: 0.8280 - val_loss: 0.6750 - val_acc: 0.8313\n",
            "Epoch 52/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.6831 - acc: 0.8276\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6680 - acc: 0.8258 - val_loss: 0.6838 - val_acc: 0.8276\n",
            "Epoch 53/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.6384 - acc: 0.8420\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6656 - acc: 0.8278 - val_loss: 0.6383 - val_acc: 0.8420\n",
            "Epoch 54/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.6594 - acc: 0.8408\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6646 - acc: 0.8273 - val_loss: 0.6597 - val_acc: 0.8408\n",
            "Epoch 55/125\n",
            "10000/10000 [==============================] - 2s 239us/sample - loss: 0.5943 - acc: 0.8575\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.6647 - acc: 0.8283 - val_loss: 0.5940 - val_acc: 0.8575\n",
            "Epoch 56/125\n",
            "10000/10000 [==============================] - 2s 239us/sample - loss: 0.5908 - acc: 0.8612\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6638 - acc: 0.8282 - val_loss: 0.5897 - val_acc: 0.8612\n",
            "Epoch 57/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.6464 - acc: 0.8413\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6641 - acc: 0.8296 - val_loss: 0.6463 - val_acc: 0.8413\n",
            "Epoch 58/125\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 0.6201 - acc: 0.8554\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.6618 - acc: 0.8287 - val_loss: 0.6201 - val_acc: 0.8554\n",
            "Epoch 59/125\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.6777 - acc: 0.8365\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6550 - acc: 0.8318 - val_loss: 0.6792 - val_acc: 0.8365\n",
            "Epoch 60/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6448 - acc: 0.8441\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6552 - acc: 0.8299 - val_loss: 0.6455 - val_acc: 0.8441\n",
            "Epoch 61/125\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.6487 - acc: 0.8447\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6539 - acc: 0.8318 - val_loss: 0.6495 - val_acc: 0.8447\n",
            "Epoch 62/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6049 - acc: 0.8582\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6589 - acc: 0.8296 - val_loss: 0.6048 - val_acc: 0.8582\n",
            "Epoch 63/125\n",
            "10000/10000 [==============================] - 2s 216us/sample - loss: 0.6109 - acc: 0.8576\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6557 - acc: 0.8314 - val_loss: 0.6110 - val_acc: 0.8576\n",
            "Epoch 64/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.6084 - acc: 0.8533\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6524 - acc: 0.8309 - val_loss: 0.6090 - val_acc: 0.8533\n",
            "Epoch 65/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.6408 - acc: 0.8445\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.6544 - acc: 0.8325 - val_loss: 0.6414 - val_acc: 0.8445\n",
            "Epoch 66/125\n",
            "10000/10000 [==============================] - 2s 237us/sample - loss: 0.6471 - acc: 0.8441\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6526 - acc: 0.8320 - val_loss: 0.6481 - val_acc: 0.8441\n",
            "Epoch 67/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6139 - acc: 0.8507\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6534 - acc: 0.8321 - val_loss: 0.6153 - val_acc: 0.8507\n",
            "Epoch 68/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.6507 - acc: 0.8391\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6469 - acc: 0.8352 - val_loss: 0.6518 - val_acc: 0.8391\n",
            "Epoch 69/125\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 0.6285 - acc: 0.8471\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6478 - acc: 0.8338 - val_loss: 0.6286 - val_acc: 0.8471\n",
            "Epoch 70/125\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 0.5935 - acc: 0.8578\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6492 - acc: 0.8342 - val_loss: 0.5940 - val_acc: 0.8578\n",
            "Epoch 71/125\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 0.6236 - acc: 0.8539\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6454 - acc: 0.8346 - val_loss: 0.6232 - val_acc: 0.8539\n",
            "Epoch 72/125\n",
            "10000/10000 [==============================] - 2s 227us/sample - loss: 0.6065 - acc: 0.8586\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6454 - acc: 0.8345 - val_loss: 0.6063 - val_acc: 0.8586\n",
            "Epoch 73/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5818 - acc: 0.8587\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.6437 - acc: 0.8358 - val_loss: 0.5818 - val_acc: 0.8587\n",
            "Epoch 74/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5883 - acc: 0.8578\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.6492 - acc: 0.8330 - val_loss: 0.5885 - val_acc: 0.8578\n",
            "Epoch 75/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.5870 - acc: 0.8643\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6460 - acc: 0.8357 - val_loss: 0.5865 - val_acc: 0.8643\n",
            "Epoch 76/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.5949 - acc: 0.8568\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6467 - acc: 0.8333 - val_loss: 0.5952 - val_acc: 0.8568\n",
            "Epoch 77/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.5541 - acc: 0.8679\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.6048 - acc: 0.8487 - val_loss: 0.5541 - val_acc: 0.8679\n",
            "Epoch 78/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.5941 - acc: 0.8616\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5805 - acc: 0.8547 - val_loss: 0.5944 - val_acc: 0.8616\n",
            "Epoch 79/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5628 - acc: 0.8670\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.5737 - acc: 0.8563 - val_loss: 0.5634 - val_acc: 0.8670\n",
            "Epoch 80/125\n",
            "10000/10000 [==============================] - 2s 221us/sample - loss: 0.5986 - acc: 0.8514\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5663 - acc: 0.8579 - val_loss: 0.5988 - val_acc: 0.8514\n",
            "Epoch 81/125\n",
            "10000/10000 [==============================] - 2s 235us/sample - loss: 0.5529 - acc: 0.8690\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5661 - acc: 0.8570 - val_loss: 0.5534 - val_acc: 0.8690\n",
            "Epoch 82/125\n",
            "10000/10000 [==============================] - 2s 238us/sample - loss: 0.5134 - acc: 0.8781\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5601 - acc: 0.8580 - val_loss: 0.5131 - val_acc: 0.8781\n",
            "Epoch 83/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.5722 - acc: 0.8624\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5498 - acc: 0.8612 - val_loss: 0.5728 - val_acc: 0.8624\n",
            "Epoch 84/125\n",
            "10000/10000 [==============================] - 2s 238us/sample - loss: 0.5652 - acc: 0.8641\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5528 - acc: 0.8595 - val_loss: 0.5652 - val_acc: 0.8641\n",
            "Epoch 85/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.5833 - acc: 0.8600\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5428 - acc: 0.8631 - val_loss: 0.5834 - val_acc: 0.8600\n",
            "Epoch 86/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5576 - acc: 0.8681\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.5421 - acc: 0.8618 - val_loss: 0.5576 - val_acc: 0.8681\n",
            "Epoch 87/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.5197 - acc: 0.8752\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.5434 - acc: 0.8611 - val_loss: 0.5197 - val_acc: 0.8752\n",
            "Epoch 88/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.5166 - acc: 0.8732\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.5323 - acc: 0.8644 - val_loss: 0.5163 - val_acc: 0.8732\n",
            "Epoch 89/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.5434 - acc: 0.8657\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5331 - acc: 0.8622 - val_loss: 0.5435 - val_acc: 0.8657\n",
            "Epoch 90/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.5320 - acc: 0.8695\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.5348 - acc: 0.8625 - val_loss: 0.5318 - val_acc: 0.8695\n",
            "Epoch 91/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5117 - acc: 0.8762\n",
            "782/782 [==============================] - 30s 38ms/step - loss: 0.5293 - acc: 0.8634 - val_loss: 0.5122 - val_acc: 0.8762\n",
            "Epoch 92/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.5136 - acc: 0.8752\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.5347 - acc: 0.8623 - val_loss: 0.5132 - val_acc: 0.8752\n",
            "Epoch 93/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.5393 - acc: 0.8679\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.5276 - acc: 0.8626 - val_loss: 0.5404 - val_acc: 0.8679\n",
            "Epoch 94/125\n",
            "10000/10000 [==============================] - 2s 221us/sample - loss: 0.5601 - acc: 0.8649\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.5254 - acc: 0.8640 - val_loss: 0.5602 - val_acc: 0.8649\n",
            "Epoch 95/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.4967 - acc: 0.8778\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5264 - acc: 0.8636 - val_loss: 0.4964 - val_acc: 0.8778\n",
            "Epoch 96/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.5188 - acc: 0.8736\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5211 - acc: 0.8641 - val_loss: 0.5189 - val_acc: 0.8736\n",
            "Epoch 97/125\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 0.5142 - acc: 0.8747\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5204 - acc: 0.8640 - val_loss: 0.5149 - val_acc: 0.8747\n",
            "Epoch 98/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.4963 - acc: 0.8781\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5219 - acc: 0.8635 - val_loss: 0.4962 - val_acc: 0.8781\n",
            "Epoch 99/125\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 0.5051 - acc: 0.8748\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5143 - acc: 0.8670 - val_loss: 0.5051 - val_acc: 0.8748\n",
            "Epoch 100/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.4881 - acc: 0.8789\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5214 - acc: 0.8646 - val_loss: 0.4885 - val_acc: 0.8789\n",
            "Epoch 101/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.5279 - acc: 0.8696\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.5135 - acc: 0.8653 - val_loss: 0.5282 - val_acc: 0.8696\n",
            "Epoch 102/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.5053 - acc: 0.8775\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4994 - acc: 0.8714 - val_loss: 0.5064 - val_acc: 0.8775\n",
            "Epoch 103/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.4961 - acc: 0.8773\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4876 - acc: 0.8734 - val_loss: 0.4970 - val_acc: 0.8773\n",
            "Epoch 104/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.4752 - acc: 0.8824\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4853 - acc: 0.8759 - val_loss: 0.4754 - val_acc: 0.8824\n",
            "Epoch 105/125\n",
            "10000/10000 [==============================] - 2s 217us/sample - loss: 0.4981 - acc: 0.8800\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4826 - acc: 0.8738 - val_loss: 0.4991 - val_acc: 0.8800\n",
            "Epoch 106/125\n",
            "10000/10000 [==============================] - 2s 231us/sample - loss: 0.4853 - acc: 0.8831\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4764 - acc: 0.8768 - val_loss: 0.4856 - val_acc: 0.8831\n",
            "Epoch 107/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.4789 - acc: 0.8830\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4705 - acc: 0.8781 - val_loss: 0.4788 - val_acc: 0.8830\n",
            "Epoch 108/125\n",
            "10000/10000 [==============================] - 2s 221us/sample - loss: 0.4861 - acc: 0.8782\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4743 - acc: 0.8780 - val_loss: 0.4861 - val_acc: 0.8782\n",
            "Epoch 109/125\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 0.5012 - acc: 0.8761\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4721 - acc: 0.8760 - val_loss: 0.5026 - val_acc: 0.8761\n",
            "Epoch 110/125\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.5145 - acc: 0.8723\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4722 - acc: 0.8767 - val_loss: 0.5160 - val_acc: 0.8723\n",
            "Epoch 111/125\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.4921 - acc: 0.8786\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.4691 - acc: 0.8781 - val_loss: 0.4929 - val_acc: 0.8786\n",
            "Epoch 112/125\n",
            "10000/10000 [==============================] - 3s 254us/sample - loss: 0.4868 - acc: 0.8806\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4624 - acc: 0.8788 - val_loss: 0.4881 - val_acc: 0.8806\n",
            "Epoch 113/125\n",
            "10000/10000 [==============================] - 2s 230us/sample - loss: 0.4856 - acc: 0.8817\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.4632 - acc: 0.8793 - val_loss: 0.4870 - val_acc: 0.8817\n",
            "Epoch 114/125\n",
            "10000/10000 [==============================] - 2s 240us/sample - loss: 0.4636 - acc: 0.8876\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4623 - acc: 0.8803 - val_loss: 0.4641 - val_acc: 0.8876\n",
            "Epoch 115/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.4875 - acc: 0.8813\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4646 - acc: 0.8788 - val_loss: 0.4878 - val_acc: 0.8813\n",
            "Epoch 116/125\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 0.5032 - acc: 0.8764\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4584 - acc: 0.8780 - val_loss: 0.5047 - val_acc: 0.8764\n",
            "Epoch 117/125\n",
            "10000/10000 [==============================] - 2s 236us/sample - loss: 0.4774 - acc: 0.8829\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4553 - acc: 0.8798 - val_loss: 0.4783 - val_acc: 0.8829\n",
            "Epoch 118/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.4853 - acc: 0.8818\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4627 - acc: 0.8771 - val_loss: 0.4864 - val_acc: 0.8818\n",
            "Epoch 119/125\n",
            "10000/10000 [==============================] - 2s 215us/sample - loss: 0.4570 - acc: 0.8874\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4595 - acc: 0.8798 - val_loss: 0.4576 - val_acc: 0.8874\n",
            "Epoch 120/125\n",
            "10000/10000 [==============================] - 2s 229us/sample - loss: 0.4975 - acc: 0.8748\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.4544 - acc: 0.8814 - val_loss: 0.4988 - val_acc: 0.8748\n",
            "Epoch 121/125\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 0.4940 - acc: 0.8812\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4510 - acc: 0.8808 - val_loss: 0.4949 - val_acc: 0.8812\n",
            "Epoch 122/125\n",
            "10000/10000 [==============================] - 2s 225us/sample - loss: 0.4611 - acc: 0.8848\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.4535 - acc: 0.8815 - val_loss: 0.4618 - val_acc: 0.8848\n",
            "Epoch 123/125\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 0.4714 - acc: 0.8798\n",
            "782/782 [==============================] - 29s 38ms/step - loss: 0.4553 - acc: 0.8792 - val_loss: 0.4721 - val_acc: 0.8798\n",
            "Epoch 124/125\n",
            "10000/10000 [==============================] - 2s 227us/sample - loss: 0.4623 - acc: 0.8826\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4517 - acc: 0.8810 - val_loss: 0.4623 - val_acc: 0.8826\n",
            "Epoch 125/125\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.4796 - acc: 0.8813\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.4494 - acc: 0.8797 - val_loss: 0.4800 - val_acc: 0.8813\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_50'), TensorSpec(shape=(16, 32, 32, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(16, 10), dtype=tf.float32, name='dense_2_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.RMSprop object at 0x7f1c490d7cc0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 7.568108797073364 secs\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.4795 - acc: 0.8817\n",
            "\n",
            "Test result: 88.170 loss: 0.479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "evAzSVkWZhVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "10. Testing"
      ]
    },
    {
      "metadata": {
        "id": "khzFCOuvZo6L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "scores = tpu_model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print(\"\\nTest result: %.3f loss: %.3f\" % (scores[1]*100,scores[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EN7g0_SXX57Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "[[1] Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
        "\n",
        "[[2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ](https://arxiv.org/abs/1502.03167)"
      ]
    }
  ]
}